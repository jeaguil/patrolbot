# -*- coding: utf-8 -*-
"""Models on Stream (No MXNet).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AhWtEIsXGqRQyslKCDoMVDZDLXigxR6T
"""

#!pip3 install torch
#!pip3 install torchvision
#!pip3 install opencv-python
#!pip3 install numpy
#!pip3 install PyYAML
#!pip3 list

# import items for object detection
import cv2
import os

#from google.colab.patches import cv2_imshow
#os.environ["OPENCV_FFMPEG_CAPTURE_OPTIONS"] = "rtsp_transport;udp"
import torch
import numpy as np

import torchvision.transforms as T

# run yolo based on video stream
def runModels(url):
    vcap = cv2.VideoCapture(url)
    vcap.set(cv2.CAP_PROP_BUFFERSIZE, 2)

    # Torch code adapted from https://github.com/akash-agni/Real-Time-Object-Detection/blob/main/Object_Detection_Youtube.py
    #model_weight_path = os.path.join(os.getcwd(), 'Documents/CS426/PatrolBot/model_weights/best.pt')
    #model = torch.hub.load('ultralytics/yolov5', 'custom', model_weight_path)
    model = torch.hub.load('ultralytics/yolov5', 'yolov5s')
    
    # Extract the names of the classes for trained the YoloV5 model
    classes = model.names
    #class_ids = [0,1,2,3]
    class_ids = [i for i in range(len(classes))]
    COLORS = np.random.uniform(0, 255, size=(len(classes), 3))

    while(True):
      # Capture frame-by-frame
      ret, frame = vcap.read()

      if ret:
    

          image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

          # Flip video frame, so it isn't reversed
          image = cv2.flip(image, 1)
          
          # If model is turned on and the object is initialized
          # run object detection on each frame
                  
          ################################################################
          #TORCH OBJECT DETECTION
          ################################################################

          # Get dimensions of the current video frame
          x_shape = image.shape[1]
          y_shape = image.shape[0]

          # Apply the Torch YoloV5 model to this frame
          results = model(image)
          
          # Extract the labels and coordinates of the bounding boxes
          #labels, cords = results.xyxyn[0][:, -1].torch.ToTensor(), results.xyxyn[0][:, :-1].torch.ToTensor()
          labels, cords = results.xyxyn[0][:, -1].tolist(), results.xyxyn[0][:, :-1].tolist()
          numberOfLabels = len(labels)
          
          # declare empty array of objects found
          objectsFound = []

          for i in range(numberOfLabels):
              row = cords[i]
              # Get the class number of current label
              class_number = int(labels[i])
              # Index colors list with current label number
              color = COLORS[class_ids[class_number]]
              
              # If confidence level is greater than 0.2
              if row[4] >= 0.4:
                  # Get label to send to dashbaord
                  label = classes[class_number]
                  x1, y1, x2, y2 = int(row[0]*x_shape), int(row[1]*y_shape), int(row[2]*x_shape), int(row[3]*y_shape)
                  print(label, " detected")
                  # append coords and label so it can be analyzed
                  objectsFound.append([x1, y1, x2, y2, label])

                  # If global enable flag is set true then show boxes
                  # Draw bounding box
                  cv2.rectangle(image, (int(x1),int(y1)), (int(x2),int(y2)), color, 2)
                  # Give bounding box a text label
                  cv2.putText(image, str(classes[int(labels[i])]), (int(x1)-10, int(y1)-10), cv2.FONT_HERSHEY_SIMPLEX, 2, color, 2)       

      # will need to instead output to dashboard.html
      
      if frame is not None:
          # Display the resulting frame
          #cv2_imshow(image)

          # Press q to close the video windows before it ends if you want
          if cv2.waitKey(22) & 0xFF == ord('q'):
              break
      else:
          print("Frame is None")
          break
          

    # When everything done, release the capture
    vcap.release()
    cv2.destroyAllWindows()
    print("Video stop")
    

if __name__ == '__main__':
    url = 'https://cph-p2p-msl.akamaized.net/hls/live/2000341/test/master.m3u8'
    # run models on URL and show output 
    runModels(url)